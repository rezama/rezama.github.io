<!DOCTYPE html>
<html lang="en">

<head>

	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Reza Mahjourian">
	<meta name="author" content="Reza Mahjourian">

	<title>Reza Mahjourian's Academic Website</title>

	<!-- Bootstrap Core CSS -->
	<link href="css/bootstrap.css" rel="stylesheet">

	<!-- Custom CSS -->
	<!--<link href="css/grayscale.css" rel="stylesheet" title="default">-->
	<link href="css/grayscale_white.css" rel="stylesheet" title="default">

	<!-- Custom Fonts -->
	<link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
	<link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
	<link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

	<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

	<!--<script type="text/javascript" src="/js/styleswitcher.js"></script>-->

	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-PEKXEDJDLK"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());
	
	  gtag('config', 'G-PEKXEDJDLK');
	</script>

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

	<!-- Navigation -->
	<nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
		<div class="container">
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
					<i class="fa fa-bars"></i>
				</button>
				<a class="navbar-brand page-scroll" href="#page-top">
					<i class="fa fa-chevron-circle-up"></i> HOME
					<!--<span class="light">Home</span>-->
				</a>
			</div>

			<!-- Collect the nav links, forms, and other content for toggling -->
			<div class="collapse navbar-collapse navbar-right navbar-main-collapse">
				<ul class="nav navbar-nav">
					<!-- Hidden li included to remove active class from about link when scrolled up past about section -->
					<li class="hidden">
						<a href="#page-top"></a>
					</li>
					<li>
						<a class="page-scroll" href="#about">About</a>
					</li>
					<li>
						<a class="page-scroll" href="#news">News</a>
					</li>
					<li>
						<a class="page-scroll" href="#research">Research</a>
					</li>
				</ul>
			</div>
			<!-- /.navbar-collapse -->
		</div>
		<!-- /.container -->
	</nav>

	<!-- Intro Header -->
	<header class="intro">
		<div class="intro-body">
			<div class="container">
				<div class="row">
					<div class="col-md-8 col-md-offset-2">
						<div class="rein-intro-bg">
							<h1 class="brand-heading">Reza Mahjourian</h1>
							<p class="intro-text">Researcher in robotics<br>and computer vision
							</p>
							<a href="mailto:firstname.lastname@gmail.com" target="_top" class="fa fa-envelope"></a>
							<a href="https://github.com/rezama?tab=repositories" target="_blank" class="fa fa-github"></a>
							<a href="https://www.linkedin.com/in/reza-mahjourian-556b17107" target="_blank" class="fa fa-linkedin"></a>
							<a href="https://www.youtube.com/c/RezaMahjourian" target="_blank" class="fa fa-youtube"></a>
							<a href="https://twitter.com/rezama" target="_blank" class="fa fa-twitter"></a>
						</div>
						<a href="#about" class="btn btn-circle page-scroll" style="margin-top:100px; color:#02d4db">
							<i class="fa fa-angle-double-down animated"></i>
						</a>
					</div>
				</div>
			</div>
		</div>
	</header>

	<!-- About Section -->
	<section id="about" class="content-section text-center">

		<div class="container">
			<div class="col-lg-8 col-lg-offset-2 text-justify">
				<img src="img/reza.jpg" class="img-circle img-me" />
				<h2 class="text-center text-padded">About</h2>
				<p>

					I'm a PhD student in the <a href="https://www.cs.utexas.edu/" target="_blank">Computer Science Department</a> at <a href="https://www.utexas.edu/" target="_blank">UT Austin</a>.  I'm interested in learning algorithms, especially in the context of robotics and computer vision.
					<br><br>

					I recently completed a Student Researcher Program at <a href="https://ai.google/research/teams/brain/robotics/"	target="_blank">Google Brain Robotics</a> in Mountain View, CA, lasting about 19 months.
					<br><br>

					I got my Master's in <a href="https://www.cise.ufl.edu/" target="_blank">Computer Science</a> from <a href="https://www.ufl.edu/" target="_blank">University of Florida</a>, working mostly on theoretical computer science and approximation algorithms. I got my Bachelor's degree in <a href="http://ce.sharif.edu/" target="_blank">Computer Engineering</a> from <a href="http://ce.sharif.edu/" target="_blank">Sharif Univerisity of Technology</a>.
					<br><br>

					Before starting graduate school, I worked as a software engineering.  Examples of my work include: lead software developer for a <a href="https://goteamup.com/" target="_blank">startup</a>, light-weight <a href="http://xpage.sf.net/" target="_blank">web framework</a> open-sourced in 2002, and <a href="https://github.com/tensorflow/tensorflow/commit/b592a8295aac0fdfffc2aa55695924e53e90bba7" target="_blank">library functions</a> contributed to <a href="https://www.tensorflow.org/" target="_blank">TensorFlow</a>.

				</p>
			</div>
		</div>

		<ul class="list-inline banner-social-buttons text-center">
			<li>
				<a href="resume.pdf#zoom=100" target="_blank" class="btn btn-default btn-lg"><i class="fa fa-file-pdf-o fa-fw"></i>
					<span class="network-name">Resume</span></a>
			</li>
		</ul>

		<br><br>

        <iframe width="560" height="315" src="https://www.youtube.com/embed/a3fjN9XTK7k?rel=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <!--
        <video width="640" height="360" controls>
			<source src="img/talk.mp4" type="video/mp4">
		    Your browser does not support the video tag.
		</video>
        -->
		<p>
			My talk on sample-efficient learning of robot table tennis in a virtual reality environment.<br>
			High-resolution videos are available on the <a href="https://sites.google.com/view/robottabletennis" target="_blank">project website</a>.
		</p>

	</section>

	<!-- News Section -->
	<section id="news" class="container content-section text-center">
		<div class="row">
			<div class="col-lg-8 col-lg-offset-2">
				<h2>News</h2>
				<ul class="left-text">
	
					<li>
						<p>
							I successfully defended my <a href="projects/toaster/phdthesis.pdf" target="_blank">PhD thesis</a>. Thanks to my adviser <a href="https://www.cs.utexas.edu/users/risto/" target="_blank">Risto Miikkulainen</a>, and my supportive committee members, <a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank">Sergey Levine</a>, <a href="https://www.ae.utexas.edu/faculty/faculty-directory/sentis" target="_blank">Luis Sentis</a>, <a href="http://www.cs.utexas.edu/~sniekum/" target="_blank">Scott Niekum</a>, <a href="http://www.cs.utexas.edu/~mok/mok.html" target="_blank">Aloysius Mok</a>.
						</p>
					</li>
					<li>
						<p>
							Our paper on <a href="https://arxiv.org/abs/1811.12927" target="_blank">sample-efficient learing of robot table	tennis</a> is on arXiv. <a href="https://sites.google.com/view/robottabletennis" target="_blank">Videos <i class="fa fa-video-camera fa-fw"></i></a>
						</p>
					</li>
					<li>
						<p>
							We have published a <a href="https://ai.googleblog.com/2018/11/a-structured-approach-to-unsupervised.html" target="_blank">Google AI blog post</a> covering our recent work on predicting object motion and depth from video.
						</p>
					</li>
					<li>
						<p>
							I completed a Student Researcher Program at <a href="https://ai.google/research/teams/brain/robotics/" target="_blank">Google Brain Robotics</a>, lasting about 19 months.
						</p>
					</li>
					<li>
						<p>
							Our follow-up work on <a href="https://arxiv.org/pdf/1811.06152.pdf" target="_blank">unsupervised depth and ego-motion prediction</a> is accepted to AAAI 2019.
						</p>
					</li>
					<li>
						<p>
							The <a href="https://github.com/tensorflow/models/tree/master/research/vid2depth" target="_blank">vid2depth codebase</a> for our upcoming <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.pdf" target="_blank">CVPR paper</a> is released in the TensorFlow Models repository.
						</p>
					</li>
					<li>
						<p>
							We have released the <a href="https://sites.google.com/site/brainrobotdata/home/bike-video-dataset" target="_blank">bike video dataset</a>. See some sample snippets from the videos on the <a href="https://sites.google.com/view/vid2depth" target="_blank">project website</a>.
						</p>
					</li>
					<li>
						<p>
							vid2depth was featured in <a href="https://youtu.be/t81QhHaMS7w?t=21m00s" target="_blank">Google I/O '18 <i class="fa fa-video-camera fa-fw"></i></a>.
						</p>
					</li>
					<li>
						<p>
							Our work on <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.pdf" target="_blank">unsupervised learning of scene depth and camera motion</a> just by observing the movement of pixels in raw monocular video (vid2depth) is accepted to <a href="http://cvpr2018.thecvf.com/" target="_blank">CVPR 2018</a> in Salt Lake City, Utah.
						</p>
					</li>
					<li>
						<p>
							I'm presenting our <a href="https://arxiv.org/pdf/1609.06377.pdf" target="_blank">video prediction</a> paper at <a href="http://http://iv2017.org/" target="_blank">IEEE Intelligent Vehicles Symposium</a> in Redondo Beach, CA.
						</p>
					</li>

				</ul>
			</div>
		</div>
	</section>

	<section id="research" class="content-section text-center">
		<div class="container">
			<div class="col-lg-8 col-lg-offset-2 text-left">
				<h2 class="text-center">Research</h2>
				<br>
				
				<!--
				<ul class="list-inline banner-social-buttons text-center">
					<li>
						<a href="https://scholar.google.com/citations?user=tpbE64QAAAAJ&hl=en" class="btn btn-default btn-lg"><i
							 class="fa fa-google fa-fw"></i> <span class="network-name">Google Scholar</span></a>
					</li>
				</ul>

				<br><br>
				-->
				<!-- ================================================================== -->

				<h3>
					Computer Vision
				</h3>
				<p class="text-justify">
					In computer vision, my focus has been on unsupervised and self-supervised learning to extract information from readily-available sources of data.  In particular, I have worked on applying deep learning and geometry to estimate scene depth and camera motion just from analyzing the movement of pixels in raw single-view videos.
				</p>
				<div style="width:100%; text-align:center;">
					<img width="416" height="384" src="projects/struct2depth/demo.gif" class="fit"/>
				</div>
				<br><br>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/struct2depth/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/1811.06152.pdf">Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos</a><br>
							AAAI, 2019<br>
							<small class="authors">
								Vincent Casser, Soeren Pirk, Reza Mahjourian, Anelia Angelova
							</small>
							<br>
							<small class="abs">
								This paper presents a refined unsupervised depth and motion prediction model that is capable of predicting depth and motion of dynamic objects in addition to the motion of the camera, all from raw single-view (monocular) video.  In addition, if multiple frames are available at inference time, a refinement process produces more accurate depth and motion estimates.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/struct2depth/struct2depth.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://sites.google.com/view/struct2depth" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/1811.06152.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://github.com/tensorflow/models/tree/master/research/struct2depth" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/future_seg/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/1811.11358.pdf">Future Semantic Segmentation Using 3D Structure</a><br>
							ECCV 3D Reconstruction meets Semantics Workshop, 2018<br>
							<small class="authors">
								Suhani Vora, Reza Mahjourian, Soeren Pirk, Anelia Angelova
							</small>
							<br>
							<small class="abs">
								Given a stream of monocular video frames sparsely labelled with semantic segmentation maps, this method estimates the 3D structure of the scene and uses that to predict the semantic segmentation of future frames.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/future_seg/future_seg.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/1811.11358.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/vid2depth/approach.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/1802.05522.pdf">Unsupervised Learning of Depth and Egomotion from Monocular Video Using 3D Geometric Constraints</a><br>
							CVPR, 2018<br>
							<small class="authors">
								Reza Mahjourian, Martin Wicke, Anelia Angelova
							</small>
							<br>
							<small class="abs">
								This paper applies deep learning and geometry to estimate scene depth and camera motion just from analyzing the movement of pixels in raw single-view videos.  The neural network estimates 3D point clouds for each frame and the camera motion between adjacent frames.  Transforming the point clouds based on the estimated camera motion and aligning them in 3D provides the supervisory signal for learning both depth and camera motion without ground truth.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/vid2depth/vid2depth.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://sites.google.com/view/vid2depth" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/1802.05522.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://github.com/tensorflow/models/tree/master/research/vid2depth" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
								<li>
									<a href="https://youtu.be/t81QhHaMS7w?t=21m00s" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-video-camera fa-fw"></i>
										<span class="network-name">Google I/O '18</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/next_frame/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/1609.06377.pdf">Geometry-Based Next Frame Prediction from Monocular Video</a><br>
							IEEE Intelligent Vehicles, 2017<br>
							<small class="authors">
								Reza Mahjourian, Martin Wicke, Anelia Angelova
							</small>
							<br>
							<small class="abs">
								A recurrent neural network with convolutional LSTM cells is trained to predict depth from a sequence of monocular video frames.  The memory in LSTM cells allows the network to The depth prediction along with the camera trajectory is then used to compute a prediction for the next frame.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/next_frame/next_frame.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/1609.06377.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>				

				

				<br><br><br><br>
				<!-- ================================================================== -->

				<h3>
					Robotics and Reinforcement Learning
				</h3>

				<p class="text-justify">
					In robotics, my focus has been on developing approaches that are sample-efficient enough that learning algorithms can be used to solve complex robotic tasks.  My work has explored <a href="projects/active_learning/document.pdf" target="_blank">active learning for robotics</a>.  I have worked on applying <a href="projects/robev/proposal.pdf" target="_blank">hierarchical learning</a> to robotics in setups where the low-level control problems are solved using optimal control and model-free learning is used only for high-level behaviors.  Our recent work on learning <a href="https://sites.google.com/view/robottabletennis" target="_blank">robot table tennis</a> is such an approach, which trains zero-shot striking skills based on dynamics models trained from observing human games in a virtual reality environment, and applies model-free reinforcement learning tactfully to discover novel game-play strategies.
					<br><br>
					In studying reinforcement learning, I have worked on understanding the properties of learning algorithms and problem domains that contribute to the success or failure of learning approaches.  I have studied the impact of domain properties like <a href="https://github.com/rezama/gamegraph" target="_blank">ergodicity and stochasticity</a> on <a href="projects/gamegraph/document.pdf" target="_blank">reinforcement learning with self-play</a>.  I have also worked on <a href="projects/discovery/paper.pdf" target="_blank">meta-learning</a> to discover effective feature sets for reinforcement learning.
				</p>
				<div style="width:100%; text-align:center;">
					<img width="620" height="360" src="projects/toaster/strategy-hitball-coop.gif" class="fit"/>
				</div>
				<br>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/toaster/self-play.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/1811.12927.pdf">Hierarchical Policy Design for Sample-Efficient Learning of Robot Table Tennis Through Self-Play</a><br>
							arXiv preprint, 2018<br>
							<small class="authors">
								Reza Mahjourian, Risto Miikkulainen, Nevena Lazic, Sergey Levine, Navdeep Jaitly
							</small>
							<br>
							<small class="abs">
								This work studies sample-efficient learning of complex policies in the context of robot table tennis.  Human demonstrations in a virtual reality environment are used to train dynamics models for the game objects, which together with an analytic paddle controller allow any robot anatomy to play table tennis without training episodes.  Self-play is used to train cooperative and adversarial game-play strategies on top of model-based striking skills trained from human demonstrations.  Further experiments demonstrate that more flexible variants of the policy can discover new strikes not demonstrated by humans and achieve higher performance at the expense of lower sample-efficiency.  The high sample-efficiency demonstrated in the evaluations show that the proposed method is suitable for learning directly on physical robots without transfer of models or policies from simulation.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/toaster/table-tennis.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/1811.12927.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://sites.google.com/view/robottabletennis" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/robev/gps-robot.gif" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://youtu.be/howSm0GAnW0">Task Planning with Guided Policy Search</a><br>
							Preprint, 2016<br>
							<small class="authors">
								Reza Mahjourian, Risto Miikkulainen
							</small>
							<br>
							<small class="abs">
								Discovering suitable cost functions allows Guided Policy Search (GPS) to solve tasks that require planning for intermediate goals. As the animation in the video shows, direct optimization may lead to local optima.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="https://youtu.be/howSm0GAnW0" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-video-camera fa-fw"></i>
										<span class="network-name">video</span></a>
								</li>
								<li>
									<a href="https://github.com/cbfinn/gps/pull/106" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/robev/manufacturing-sim.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/robev/proposal.pdf">Neuroevolutionary Planning for Robotic Control</a><br>
							PhD Proposal, 2016<br>
							<small class="authors">
								Reza Mahjourian, Risto Miikkulainen
							</small>
							<br>
							<small class="abs">
								In this work, an evolutionary strategy is applied to discover robotic controllers for an object manipulation task.  <!--The experiments show that moderate amounts of actuation noise improve the controller’s robustness against a joint malfunction.  Interestingly, using actuation noise at training time improves learning even in normal conditions where there is no malfunction at test time.-->

								For simple control tasks, controllers with precise behavior are learned. However, when the task is complex enough that it require strategy and planning, finding solutions becomes hard.  This work proposes a new evolutionary method to discover and complete subtasks leading to completion of an original objective.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/robev/proposal.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/robev/proposal.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/robev/drill.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/robev/poster.pdf">Robotic Control Through Neuroevolution</a><br>
							BEACON, 2014<br>
							<small class="authors">
								Reza Mahjourian, Risto Miikkulainen
							</small>
							<br>
							<small class="abs">
								This work studies the impact of neural network architecture on efficiency of neuroevolution (<a href="https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies" target="_blank">NEAT</a>) on object manipulation tasks using the Atlas robot.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/robev/beacon.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/robev/poster.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/discovery/keepaway.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/discovery/paper.pdf">An Evolutionary Feature Discovery Method for Reinforcement Learning</a><br>
							GECCO submission, 2013<br>
							<small class="authors">
								Reza Mahjourian, Peter Stone
							</small>
							<br>
							<small class="abs">
								This work presents a meta-learning approach for generating and evaluating candidate feature sets for reinforcement learning with linear function approximators (Gradient-Descent Sarsa(λ)).
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/discovery/discovery.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/discovery/paper.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://github.com/rezama/discovery" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/gamegraph/backgammon.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/gamegraph/document.pdf">Studying Impact of Domain Ergodicity and Stochasticity on Reinforcement Learning with Self-Play</a><br>
							Preprint, 2011<br>
							<small class="authors">
								Reza Mahjourian, Prateek Maheshwari, Risto Miikkulainen
							</small>
							<br>
							<small class="abs">
								This work studies hypotheses on why reinforcement learning worked so well for backgammon in <a href="http://www.bkgm.com/articles/tesauro/tdl.html" target="_blank">TD-Gammon</a>.  Does backgammon have particular properties that make it easier for reinforcement learning and self-play to work?  Can these properties be exploited to design better general learning algorithms?  Follow-up experiments show domain stochasticity to have a strong impact on reinforcement learning with self-play.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/gamegraph/gamegraph.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/gamegraph/document.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="projects/gamegraph/proposal.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">proposal</span></a>
								</li>
								<li>
									<a href="https://github.com/rezama/gamegraph" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/active_learning/variance.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/active_learning/document.pdf">Optimizing Selection of Training Samples for Robotics Learning Problems</a><br>
							Preprint, 2011<br>
							<small class="authors">
								Reza Mahjourian, Peter Stone
							</small>
							<br>
							<small class="abs">
								Uses an ensemble of neural networks and selects samples by prioritizing data points where the networks in the ensemble disagree the most about predictions (most variance).
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/active_learning/document.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://github.com/rezama/active-learning" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<br><br><br><br>
				<!-- ================================================================== -->

				<h3>
					Theoretical Computer Science
				</h3>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/bcast/heuristic.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/bcast/bcast.pdf">An Approximation Algorithm for Conflict-Aware Broadcast Scheduling in Wireless
								Ad Hoc Networks</a><br>
							The ACM International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc), 2008<br>
							<small class="authors">
								Reza Mahjourian, Feng Chen, Ravi Itwari, My Thai, Hongqiang Zhai, Yuguang Fang
							</small>
							<br>
							<small class="abs">
								This paper introduces and proves correctness of a constant approximation algorithm for minimum-latency conflict-aware broadcast scheduling in wireless networks. A constant approximation algorithm is a polynomial-time solution to an NP-hard problem such that the solution is within a constant multiple of the optimal solution to the problem.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/bcast/bcast.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/bcast/bcast.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<br><br><br><br>
				<!-- ================================================================== -->

				<h3>
					Software Engineering
				</h3>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/xpage/data.png" class="thumb-paper" />
						</th>
						<th class="entry">

							<a href="projects/xpage/icsr.pdf">An Architectural Style for Data-Driven Systems</a><br>
							International Conference on Software Reuse (ICSR), 2008<br>
							<small class="authors">
								Reza Mahjourian
							</small>
							<br>
							<small class="abs">
								This paper describes the design of <a href="https://xpage.sourceforge.net/" target="_blank">XPage</a>, a light-weight web application framework, which is also published as open-source software in 2002, and deployed in six data management apps by the author.  It is designed specifically for data management applications and allows the developer to specify each application page at a very high level by specifying the data sources and attributes that it retrieves or modifies.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/xpage/xpage.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://xpage.sourceforge.net/" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
								<li>
									<a href="projects/xpage/icsr.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="projects/xpage/extended.pdf" targwhich transparently translates high-level web page specs into server-side s     et="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">extended</span></a>
								</li>
								<li>
									<a href="https://sourceforge.net/projects/xpage/" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">sourceforge</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/neno/connector.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/neno/iwicss07.pdf">Software Connector Classification and Selection for Data-Intensive Systems</a><br>
							International Workshop on Incorporating COTS Software into Software Systems, 2007<br>
							<small class="authors">
								Chris A. Mattmann, David Woollard, Nenad Medvidovic, Reza Mahjourian
							</small>
							<br>
							<small class="abs">
								This work explores the role of software connectors in systems specifically designed for distributing large volumes of data.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/neno/cite.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/neno/iwicss07.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

			</div>
		</div>
	</section>

	<!-- jQuery -->
	<script src="js/jquery.js"></script>

	<!-- Bootstrap Core JavaScript -->
	<script src="js/bootstrap.min.js"></script>

	<!-- Plugin JavaScript -->
	<script src="js/jquery.easing.min.js"></script>

	<!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/
    <script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCRngKslUGJTlibkQ3FkfTxj3Xss1UlZDA&sensor=false"></script>-->

	<!-- Custom Theme JavaScript -->
	<script src="js/grayscale.js"></script>


</body>

</html>
