<!DOCTYPE html>
<html lang="en">

<head>

	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Reza Mahjourian">
	<meta name="author" content="Reza Mahjourian">

	<title>Reza Mahjourian's Academic Website</title>

	<!-- Bootstrap Core CSS -->
	<link href="css/bootstrap.css" rel="stylesheet">

	<!-- Custom CSS -->
	<!--<link href="css/grayscale.css" rel="stylesheet" title="default">-->
	<link href="css/grayscale_white.css" rel="stylesheet" title="default">

	<!-- Custom Fonts -->
	<link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
	<link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
	<link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

	<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

	<!--<script type="text/javascript" src="/js/styleswitcher.js"></script>-->

	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-PEKXEDJDLK"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());
	
	  gtag('config', 'G-PEKXEDJDLK');
	</script>

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

	<!-- Navigation -->
	<nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
		<div class="container">
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
					<i class="fa fa-bars"></i>
				</button>
				<a class="navbar-brand page-scroll" href="#page-top">
					<i class="fa fa-chevron-circle-up"></i> HOME
					<!--<span class="light">Home</span>-->
				</a>
			</div>

			<!-- Collect the nav links, forms, and other content for toggling -->
			<div class="collapse navbar-collapse navbar-right navbar-main-collapse">
				<ul class="nav navbar-nav">
					<!-- Hidden li included to remove active class from about link when scrolled up past about section -->
					<li class="hidden">
						<a href="#page-top"></a>
					</li>
					<li>
						<a class="page-scroll" href="#about">About</a>
					</li>
					<li>
						<a class="page-scroll" href="#news">News</a>
					</li>
					<li>
						<a class="page-scroll" href="#research">Research</a>
					</li>
				</ul>
			</div>
			<!-- /.navbar-collapse -->
		</div>
		<!-- /.container -->
	</nav>

	<!-- Intro Header -->
	<header class="intro">
		<div class="intro-body">
			<div class="container">
				<div class="row">
					<div class="col-md-8 col-md-offset-2">
						<div class="rein-intro-bg">
							<h1 class="brand-heading">Reza Mahjourian</h1>
							<p class="intro-text">Researcher in robotics<br>and computer vision
							</p>
							<a href="mailto:firstname.lastname@gmail.com" target="_top" class="fa fa-envelope"></a>
							<a href="https://github.com/rezama?tab=repositories" target="_blank" class="fa fa-github"></a>
							<a href="https://www.linkedin.com/in/reza-mahjourian-556b17107" target="_blank" class="fa fa-linkedin"></a>
							<a href="https://www.youtube.com/c/RezaMahjourian" target="_blank" class="fa fa-youtube"></a>
							<a href="https://twitter.com/rezama" target="_blank" class="fa fa-twitter"></a>
						</div>
						<a href="#about" class="btn btn-circle page-scroll" style="margin-top:100px; color:#02d4db">
							<i class="fa fa-angle-double-down animated"></i>
						</a>
					</div>
				</div>
			</div>
		</div>
	</header>

	<!-- About Section -->
	<section id="about" class="content-section text-center">

		<div class="container">
			<div class="col-lg-8 col-lg-offset-2 text-justify">
				<img src="img/reza.jpg" class="img-circle img-me" />
				<h2 class="text-center text-padded">About</h2>
				<p>

					I have been a Research Scientist at <a href="https://waymo.com/research/"	target="_blank">Waymo Research</a> since 2019.
					<br><br>

					I was a Student Researcher at <a href="https://ai.google/research/teams/brain/robotics/"	target="_blank">Google Brain Robotics</a> in Mountain View, CA, for about 2 years.  I joined Google as an intern 4 times.
					<br><br>

					I got my PhD in <a href="https://www.cs.utexas.edu/" target="_blank">Computer Science</a> from <a href="https://www.utexas.edu/" target="_blank">UT Austin</a>, where I worked on reinforcement learning and robotics.	I got my Master's in <a href="https://www.cise.ufl.edu/" target="_blank">Computer Science</a> from <a href="https://www.ufl.edu/" target="_blank">University of Florida</a>, working mostly on theoretical computer science and approximation algorithms. I got my Bachelor's degree in <a href="http://ce.sharif.edu/" target="_blank">Computer Engineering</a> from <a href="http://ce.sharif.edu/" target="_blank">Sharif Univerisity of Technology</a>.
					<br><br>

					Before graduate school, I worked as a software engineering.  I have been a lead software developer for a <a href="https://goteamup.com/" target="_blank">startup</a>, co-founded a startup developing custom database applications for large companies including automakers, and created a light-weight <a href="http://xpage.sf.net/" target="_blank">web framework</a> open-sourced in 2002.  

				</p>
			</div>
		</div>

		<ul class="list-inline banner-social-buttons text-center">
			<li>
				<a href="resume.pdf#zoom=100" target="_blank" class="btn btn-default btn-lg"><i class="fa fa-file-pdf-o fa-fw"></i>
					<span class="network-name">Resume</span></a>
			</li>
		</ul>

		<br><br>

        <iframe width="560" height="315" src="https://www.youtube.com/embed/a3fjN9XTK7k?rel=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <!--
        <video width="640" height="360" controls>
			<source src="img/talk.mp4" type="video/mp4">
		    Your browser does not support the video tag.
		</video>
        -->
		<p>
			My talk on sample-efficient learning of robot table tennis in a virtual reality environment.<br>
			High-resolution videos are available on the <a href="https://sites.google.com/view/robottabletennis" target="_blank">project website</a>.
		</p>

	</section>

	<!-- News Section -->
	<section id="news" class="container content-section text-center">
		<div class="row">
			<div class="col-lg-8 col-lg-offset-2">
				<h2>News</h2>
				<ul class="left-text">
	
					<li>
						<p>
							I am organizing the <a href="https://waymo.com/open/challenges/2025/scenario-generation/" target="_blank">Waymo Open Dataset Scenario Generation Challenge</a> at CVPR 2025.  Participants generate novel traffic scenarios with new agents and their trajectories.
						</p>
					</li>

					<li>
						<p>
							Our paper <a href="https://openreview.net/pdf?id=g6eNn2IOnk">Achieving Human Level Competitive Robot Table Tennis</a> was nominated for Best Robot Learning paper at ICRA 2025.
									<a href="https://sites.google.com/view/competitive-robot-table-tennis/" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i><span class="network-name">website</span></a>
									<a href="https://youtu.be/EqQl-JQxToE" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-video-camera fa-fw"></i><span class="network-name">video</span></a>
							The final iteration of this work uses a hierarchical and modular policy design as originally developed in my paper on <a href="https://arxiv.org/pdf/1811.12927.pdf">Hierarchical Policy Design for Sample-Efficient Learning of Robot Table Tennis Through Self-Play</a>.
					</li>

					<li>
						<p>
							Our paper <a href="https://arxiv.org/pdf/2407.12345">VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions</a> was accepted at ECCV 2024. <a href="https://moonseokha.github.io/VisionTrap/" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
						</p>
					</li>

					<li>
						<p>
							I am organizing the <a href="https://waymo.com/open/challenges/2022/occupancy-flow-prediction-challenge/" target="_blank">Waymo Open Dataset Occupancy and Flow Prediction Challenge</a> at CVPR 2022 and CVPR 2024.  Participants predict future occupancy and motion flow of currently-observed and currently-occluded agents.
						</p>
					</li>
					
					<hr/>

					<li>
						<p>
							I successfully defended my <a href="projects/toaster/phdthesis.pdf" target="_blank">PhD thesis</a>. Thanks to my adviser <a href="https://www.cs.utexas.edu/users/risto/" target="_blank">Risto Miikkulainen</a>, and my supportive committee members, <a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank">Sergey Levine</a>, <a href="https://www.ae.utexas.edu/faculty/faculty-directory/sentis" target="_blank">Luis Sentis</a>, <a href="http://www.cs.utexas.edu/~sniekum/" target="_blank">Scott Niekum</a>, <a href="http://www.cs.utexas.edu/~mok/mok.html" target="_blank">Aloysius Mok</a>.
						</p>
					</li>
					<li>
						<p>
							Our paper on <a href="https://arxiv.org/abs/1811.12927" target="_blank">sample-efficient learing of robot table	tennis</a> is on arXiv. <a href="https://sites.google.com/view/robottabletennis" target="_blank">Videos <i class="fa fa-video-camera fa-fw"></i></a>
						</p>
					</li>
					<li>
						<p>
							We have published a <a href="https://ai.googleblog.com/2018/11/a-structured-approach-to-unsupervised.html" target="_blank">Google AI blog post</a> covering our recent work on predicting object motion and depth from video.
						</p>
					</li>
					<li>
						<p>
							I completed a Student Researcher Program at <a href="https://ai.google/research/teams/brain/robotics/" target="_blank">Google Brain Robotics</a>, lasting about 19 months.
						</p>
					</li>
					<li>
						<p>
							Our follow-up work on <a href="https://arxiv.org/pdf/1811.06152.pdf" target="_blank">unsupervised depth and ego-motion prediction</a> is accepted to AAAI 2019.
						</p>
					</li>
					<li>
						<p>
							The <a href="https://github.com/tensorflow/models/tree/master/research/vid2depth" target="_blank">vid2depth codebase</a> for our upcoming <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.pdf" target="_blank">CVPR paper</a> is released in the TensorFlow Models repository.
						</p>
					</li>
					<li>
						<p>
							We have released the <a href="https://sites.google.com/site/brainrobotdata/home/bike-video-dataset" target="_blank">bike video dataset</a>. See some sample snippets from the videos on the <a href="https://sites.google.com/view/vid2depth" target="_blank">project website</a>.
						</p>
					</li>
					<li>
						<p>
							vid2depth was featured in <a href="https://youtu.be/t81QhHaMS7w?t=21m00s" target="_blank">Google I/O '18 <i class="fa fa-video-camera fa-fw"></i></a>.
						</p>
					</li>
					<li>
						<p>
							Our work on <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.pdf" target="_blank">unsupervised learning of scene depth and camera motion</a> just by observing the movement of pixels in raw monocular video (vid2depth) is accepted to <a href="http://cvpr2018.thecvf.com/" target="_blank">CVPR 2018</a> in Salt Lake City, Utah.
						</p>
					</li>
					
<!--

					<li>
						<p>
							I'm presenting our <a href="https://arxiv.org/pdf/1609.06377.pdf" target="_blank">video prediction</a> paper at <a href="http://http://iv2017.org/" target="_blank">IEEE Intelligent Vehicles Symposium</a> in Redondo Beach, CA.
						</p>
					</li>

-->

				</ul>
			</div>
		</div>
	</section>

	<section id="research" class="content-section text-center">
		<div class="container">
			<div class="col-lg-8 col-lg-offset-2 text-left">
				<h2 class="text-center">Research</h2>
				<br>
				
				
				<ul class="list-inline banner-social-buttons text-center">
					<li>
						<a href="https://scholar.google.com/citations?user=tpbE64QAAAAJ&hl=en" class="btn btn-default btn-lg"><i
							 class="fa fa-google fa-fw"></i> <span class="network-name">Google Scholar</span></a>
					</li>
				</ul>

				<br><br>
				
				<!-- ================================================================== -->

				<h3>
					Autonomous Vehicles
				</h3>
				
				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/visiontrap/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/2407.12345">VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions</a><br>
							ECCV, 2024<br>
							<small class="authors">
								Seokha Moon, Hyun Woo, Hongbeen Park, Haeji Jung, Reza Mahjourian, Hyung-gun Chi, Hyerin Lim, Sangpil Kim, Jinkyu Kim
							</small>
							<br>
							<small class="abs">
								Established trajectory prediction methods primarily use agent tracks generated by a detection and tracking system and HD map as inputs. In this work, we propose a novel method that also incorporates visual input from surround-view cameras.  We use textual descriptions generated by a Vision-Language Model (VLM) and refined by a Large Language Model (LLM) as supervision.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/visiontrap/visiontrap.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/2407.12345https://arxiv.org/pdf/2407.12345" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://moonseokha.github.io/VisionTrap/" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
								<li>
									<a href="https://drive.google.com/file/d/1v_M_OuLnDzRo2uXyOrDfHNHbtoIcR3RA/" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">dataset</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/unigen/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/2405.03807">UniGen: Unified Modeling of Initial Agent States and Trajectories for Generating Autonomous Driving Scenarios</a><br>
							ICRA, 2024<br>
							<small class="authors">
								Reza Mahjourian*, Rongbing Mu*, Valerii Likhosherstov, Paul Mougin, Xiukun Huang, Joao Messias, Shimon Whiteson
							</small>
							<br>
							<small class="abs">
								This paper presents a novel approach on generating new traffic scenarios for evaluating and improving autonomous driving software.  Our unified modeling approach, combined with autoregressive agent injection, conditions the placement and motion trajectory of every new agent on all existing agents and their trajectories, leading to realistic scenarios.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/unigen/unigen.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/2405.03807" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/bevmap/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://openaccess.thecvf.com/content/WACV2024/papers/Chang_BEVMap_Map-Aware_BEV_Modeling_for_3D_Perception_WACV_2024_paper.pdf">BEVMap: Map-Aware BEV Modeling for 3D Perception</a><br>
							WACV, 2024<br>
							<small class="authors">
								Mincheol Chang, Seokha Moon, Reza Mahjourian, Jinkyu Kim
							</small>
							<br>
							<small class="abs">
								We introduce a method for incorporating map information to improve perspective depth estimation from 2D camera images and thereby producing geometrically- and semantically-robust BEV features. We show that augmenting the camera images with the BEV map and map-to-camera projections can compensate for the depth uncertainty.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/bevmap/bevmap.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://openaccess.thecvf.com/content/WACV2024/papers/Chang_BEVMap_Map-Aware_BEV_Modeling_for_3D_Perception_WACV_2024_paper.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://github.com/mincheoree/BEVMap" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
								<li>
									<a href="https://youtu.be/PLeWBx-J58Q" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-video-camera fa-fw"></i>
										<span class="network-name">video</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/occflow/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/2203.03875">Occupancy Flow Fields for Motion Forecasting in Autonomous Driving</a><br>
							RA-L and ICRA, 2022<br>
							<small class="authors">
								Reza Mahjourian*, Jinkyu Kim*, Yuning Chai, Mingxing Tan, Ben Sapp, Dragomir Anguelov
							</small>
							<br>
							<small class="abs">
								We propose Occupancy Flow Fields, a new representation for motion forecasting of multiple agents, an important task in autonomous driving.  In addition, we introduce the problem of predicting speculative agents, which are currently-occluded agents that may appear in the future through dis-occlusion or by entering the field of view.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/occflow/occflow.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/2203.03875" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

        <table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/stopnet/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/2206.00991">StopNet: Scalable Trajectory and Occupancy Prediction for Urban Autonomous Driving</a><br>
							ICRA, 2022<br>
							<small class="authors">
								Jinkyu Kim*, Reza Mahjourian*, Scott Ettinger, Mayank Bansal, Brandyn White, Ben Sapp, Dragomir Anguelov
							</small>
							<br>
							<small class="abs">
								We introduce a motion forecasting (behavior prediction) method that meets the latency requirements for autonomous driving in dense urban environments without sacrificing accuracy. A whole-scene sparse input representation allows StopNet to scale to predicting trajectories for hundreds of road agents with reliable latency.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/stopnet/stopnet.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/2206.00991" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>
				
				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/cbp/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/2104.09959">Identifying Driver Interactions via Conditional Behavior Prediction</a><br>
							ICRA, 2021<br>
							<small class="authors">
								Ekaterina Tolstaya, Reza Mahjourian, Carlton Downey, Balakrishnan Varadarajan, Benjamin Sapp, Dragomir Anguelov
							</small>
							<br>
							<small class="abs">
								We develop end-to-end models for conditional behavior prediction (CBP) that take as an input a query future trajectory for an ego-agent, and predict distributions over future trajectories for other agents conditioned on the query. Leveraging such a model, we develop a general-purpose agent interactivity score derived from probabilistic first principles. The interactivity score allows us to find interesting interactive scenarios for training and evaluating behavior prediction models.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/cbp/cbp.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/2104.09959" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<h3>
					Computer Vision
				</h3>
				<p class="text-justify">
					In computer vision, my focus has been on unsupervised and self-supervised learning to extract information from readily-available sources of data.  In particular, I have worked on applying deep learning and geometry to estimate scene depth and camera motion just from analyzing the movement of pixels in raw single-view videos.
				</p>
				<div style="width:100%; text-align:center;">
					<img width="416" height="384" src="projects/struct2depth/demo.gif" class="fit"/>
				</div>
				<br><br>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/consistent_seg/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/2210.08113">Instance Segmentation with Cross-Modal Consistency</a><br>
							IROS, 2022<br>
							<small class="authors">
								Alex Zihao Zhu, Vincent Casser, Reza Mahjourian, Henrik Kretzschmar, Sören Pirk
							</small>
							<br>
							<small class="abs">
								We introduce a novel approach to instance segmentation that jointly leverages measurements from multiple sensor modalities, such as cameras and LiDAR. Our method learns to predict embeddings for each pixel or point that give rise to a dense segmentation of the scene. Specifically, our technique applies contrastive learning to points in the scene both across sensor modalities and the temporal domain. We demonstrate that this formulation encourages the models to learn embeddings that are invariant to viewpoint variations and consistent across sensor modalities.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/consistent_seg/consistent_seg.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/2210.08113" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/eseg/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/2203.12683">Revisiting Multi-Scale Feature Fusion for Semantic Segmentation</a><br>
							ArXiv, 2022<br>
							<small class="authors">
								Tianjian Meng, Golnaz Ghiasi, Reza Mahjourian, Quoc V. Le, Mingxing Tan
							</small>
							<br>
							<small class="abs">
								We develop a simplified segmentation model, named ESeg, which has neither high internal resolution nor expensive atrous convolutions. Perhaps surprisingly, our simple method can achieve better accuracy with faster speed than prior art across multiple datasets.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/eseg/eseg.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/2203.12683" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/struct2depth/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/1811.06152.pdf">Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos</a><br>
							AAAI, 2019<br>
							<small class="authors">
								Vincent Casser, Soeren Pirk, Reza Mahjourian, Anelia Angelova
							</small>
							<br>
							<small class="abs">
								This paper presents a refined unsupervised depth and motion prediction model that is capable of predicting depth and motion of dynamic objects in addition to the motion of the camera, all from raw single-view (monocular) video.  In addition, if multiple frames are available at inference time, a refinement process produces more accurate depth and motion estimates.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/struct2depth/struct2depth.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://sites.google.com/view/struct2depth" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/1811.06152.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://github.com/tensorflow/models/tree/master/research/struct2depth" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/future_seg/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/1811.11358.pdf">Future Semantic Segmentation Using 3D Structure</a><br>
							ECCV 3D Reconstruction meets Semantics Workshop, 2018<br>
							<small class="authors">
								Suhani Vora, Reza Mahjourian, Soeren Pirk, Anelia Angelova
							</small>
							<br>
							<small class="abs">
								Given a stream of monocular video frames sparsely labelled with semantic segmentation maps, this method estimates the 3D structure of the scene and uses that to predict the semantic segmentation of future frames.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/future_seg/future_seg.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/1811.11358.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/vid2depth/approach.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/1802.05522.pdf">Unsupervised Learning of Depth and Egomotion from Monocular Video Using 3D Geometric Constraints</a><br>
							CVPR, 2018<br>
							<small class="authors">
								Reza Mahjourian, Martin Wicke, Anelia Angelova
							</small>
							<br>
							<small class="abs">
								This paper applies deep learning and geometry to estimate scene depth and camera motion just from analyzing the movement of pixels in raw single-view videos.  The neural network estimates 3D point clouds for each frame and the camera motion between adjacent frames.  Transforming the point clouds based on the estimated camera motion and aligning them in 3D provides the supervisory signal for learning both depth and camera motion without ground truth.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/vid2depth/vid2depth.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://sites.google.com/view/vid2depth" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/1802.05522.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://github.com/tensorflow/models/tree/master/research/vid2depth" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
								<li>
									<a href="https://youtu.be/t81QhHaMS7w?t=21m00s" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-video-camera fa-fw"></i>
										<span class="network-name">Google I/O '18</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/next_frame/thumb.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/1609.06377.pdf">Geometry-Based Next Frame Prediction from Monocular Video</a><br>
							IEEE Intelligent Vehicles, 2017<br>
							<small class="authors">
								Reza Mahjourian, Martin Wicke, Anelia Angelova
							</small>
							<br>
							<small class="abs">
								A recurrent neural network with convolutional LSTM cells is trained to predict depth from a sequence of monocular video frames.  The memory in LSTM cells allows the network to The depth prediction along with the camera trajectory is then used to compute a prediction for the next frame.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/next_frame/next_frame.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/1609.06377.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>				

				

				<br><br><br><br>
				<!-- ================================================================== -->

				<h3>
					Robotics and Reinforcement Learning
				</h3>

				<p class="text-justify">
					In robotics, my focus has been on developing approaches that are sample-efficient enough that learning algorithms can be used to solve complex robotic tasks.  My work has explored <a href="projects/active_learning/document.pdf" target="_blank">active learning for robotics</a>.  I have worked on applying <a href="projects/robev/proposal.pdf" target="_blank">hierarchical learning</a> to robotics in setups where the low-level control problems are solved using optimal control and model-free learning is used only for high-level behaviors.  Our recent work on learning <a href="https://sites.google.com/view/robottabletennis" target="_blank">robot table tennis</a> is such an approach, which trains zero-shot striking skills based on dynamics models trained from observing human games in a virtual reality environment, and applies model-free reinforcement learning tactfully to discover novel game-play strategies.
					<br><br>
					In studying reinforcement learning, I have worked on understanding the properties of learning algorithms and problem domains that contribute to the success or failure of learning approaches.  I have studied the impact of domain properties like <a href="https://github.com/rezama/gamegraph" target="_blank">ergodicity and stochasticity</a> on <a href="projects/gamegraph/document.pdf" target="_blank">reinforcement learning with self-play</a>.  I have also worked on <a href="projects/discovery/paper.pdf" target="_blank">meta-learning</a> to discover effective feature sets for reinforcement learning.
				</p>
				<div style="width:100%; text-align:center;">
					<img width="800" height="485" src="projects/human_level/thumb.gif" class="fit"/>
				</div>
				<br>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/human_level/thumb.gif" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://openreview.net/pdf?id=g6eNn2IOnk">Achieving Human Level Competitive Robot Table Tennis</a><br>
							ICRA 2025 Best Robot Learning Paper Finalist, CoRL Workshop 2024<br>
							<small class="authors">
								David B. D’Ambrosio, Jonathan Abelian, Saminda Abeyruwan, Michael Ahn, Alex Bewley, Justin Boyd, Krzysztof Choromanski, Omar Cortes†, Erwin Coumans, Tianli Ding, Wenbo Gao, Laura Graesser, Atil Iscen, Navdeep Jaitly, Deepali Jain, Juhana Kangaspunta, Satoshi Kataoka, Gus Kouretas, Yuheng Kuang, Nevena Lazic, Corey Lynch, Reza Mahjourian, Sherry Q. Moore, Thinh Nguyen, Ken Oslund, Barney J Reed, Krista Reymann, Pannag R. Sanketi, Anish Shankar,
Pierre Sermanet, Vikas Sindhwani, Avi Singh, Vincent Vanhoucke, Grace Vesom, Peng Xu
							</small>
							<br>
							<small class="abs">
								We present the first learned robot agent that reaches amateur human-level performance in competitive table tennis. We contribute a hierarchical and modular policy architecture consisting of (i) low level controllers with their skill descriptors that model their capabilities and (ii) a high level controller that chooses the low level skills. 
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/human_level/human_level.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://openreview.net/pdf?id=g6eNn2IOnk" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://sites.google.com/view/competitive-robot-table-tennis/" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
								<li>
									<a href="https://youtu.be/EqQl-JQxToE" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-video-camera fa-fw"></i>
										<span class="network-name">video</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/high_speed_robotics/thumb.gif" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/2309.03315">Robotic Table Tennis: A Case Study into a High Speed Learning System</a><br>
							RSS, 2023<br>
							<small class="authors">
								David B D’Ambrosio, Navdeep Jaitly, Vikas Sindhwani, Ken Oslund, Peng Xu, Nevena Lazic, Anish Shankar, Tianli Ding, Jonathan Abelian, Erwin Coumans, Gus Kouretas, Thinh Nguyen, Justin Boyd, Atil Iscen, Reza Mahjourian, Vincent Vanhoucke, Alex Bewley, Yuheng Kuang, Michael Ahn, Deepali Jain, Satoshi Kataoka, Omar E Cortes, Pierre Sermanet, Corey Lynch, Pannag R Sanketi, Krzysztof Choromanski, Wenbo Gao, Juhana Kangaspunta, Krista Reymann, Grace Vesom, Sherry Q Moore, Avi Singh, Saminda W Abeyruwan, Laura Graesser
							</small>
							<br>
							<small class="abs">
								XXXXX
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/high_speed_robotics/high_speed_robotics.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/2309.03315" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://sites.google.com/view/robotictabletennis" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
								<li>
									<a href="https://youtu.be/uFcnWjB42I0" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-video-camera fa-fw"></i>
										<span class="network-name">video</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->				

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/toaster/strategy-hitball-coop.gif" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://arxiv.org/pdf/1811.12927.pdf">Hierarchical Policy Design for Sample-Efficient Learning of Robot Table Tennis Through Self-Play</a><br>
							arXiv preprint, 2018<br>
							<small class="authors">
								Reza Mahjourian, Risto Miikkulainen, Nevena Lazic, Sergey Levine, Navdeep Jaitly
							</small>
							<br>
							<small class="abs">
								This work studies sample-efficient learning of complex policies in the context of robot table tennis.  Human demonstrations in a virtual reality environment are used to train dynamics models for the game objects, which together with an analytic paddle controller allow any robot anatomy to play table tennis without training episodes.  Self-play is used to train cooperative and adversarial game-play strategies on top of model-based striking skills trained from human demonstrations.  Further experiments demonstrate that more flexible variants of the policy can discover new strikes not demonstrated by humans and achieve higher performance at the expense of lower sample-efficiency.  The high sample-efficiency demonstrated in the evaluations show that the proposed method is suitable for learning directly on physical robots without transfer of models or policies from simulation.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/toaster/table-tennis.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://arxiv.org/pdf/1811.12927.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://sites.google.com/view/robottabletennis" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/robev/gps-robot.gif" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="https://youtu.be/howSm0GAnW0">Task Planning with Guided Policy Search</a><br>
							Preprint, 2016<br>
							<small class="authors">
								Reza Mahjourian, Risto Miikkulainen
							</small>
							<br>
							<small class="abs">
								Discovering suitable cost functions allows Guided Policy Search (GPS) to solve tasks that require planning for intermediate goals. As the animation in the video shows, direct optimization may lead to local optima.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="https://youtu.be/howSm0GAnW0" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-video-camera fa-fw"></i>
										<span class="network-name">video</span></a>
								</li>
								<li>
									<a href="https://github.com/cbfinn/gps/pull/106" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/robev/manufacturing-sim.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/robev/proposal.pdf">Neuroevolutionary Planning for Robotic Control</a><br>
							PhD Proposal, 2016<br>
							<small class="authors">
								Reza Mahjourian, Risto Miikkulainen
							</small>
							<br>
							<small class="abs">
								In this work, an evolutionary strategy is applied to discover robotic controllers for an object manipulation task.  <!--The experiments show that moderate amounts of actuation noise improve the controller’s robustness against a joint malfunction.  Interestingly, using actuation noise at training time improves learning even in normal conditions where there is no malfunction at test time.-->

								For simple control tasks, controllers with precise behavior are learned. However, when the task is complex enough that it require strategy and planning, finding solutions becomes hard.  This work proposes a new evolutionary method to discover and complete subtasks leading to completion of an original objective.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/robev/proposal.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/robev/proposal.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/robev/drill.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/robev/poster.pdf">Robotic Control Through Neuroevolution</a><br>
							BEACON, 2014<br>
							<small class="authors">
								Reza Mahjourian, Risto Miikkulainen
							</small>
							<br>
							<small class="abs">
								This work studies the impact of neural network architecture on efficiency of neuroevolution (<a href="https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies" target="_blank">NEAT</a>) on object manipulation tasks using the Atlas robot.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/robev/beacon.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/robev/poster.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/discovery/keepaway.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/discovery/paper.pdf">An Evolutionary Feature Discovery Method for Reinforcement Learning</a><br>
							GECCO submission, 2013<br>
							<small class="authors">
								Reza Mahjourian, Peter Stone
							</small>
							<br>
							<small class="abs">
								This work presents a meta-learning approach for generating and evaluating candidate feature sets for reinforcement learning with linear function approximators (Gradient-Descent Sarsa(λ)).
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/discovery/discovery.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/discovery/paper.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://github.com/rezama/discovery" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/gamegraph/backgammon.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/gamegraph/document.pdf">Studying Impact of Domain Ergodicity and Stochasticity on Reinforcement Learning with Self-Play</a><br>
							Preprint, 2011<br>
							<small class="authors">
								Reza Mahjourian, Prateek Maheshwari, Risto Miikkulainen
							</small>
							<br>
							<small class="abs">
								This work studies hypotheses on why reinforcement learning worked so well for backgammon in <a href="http://www.bkgm.com/articles/tesauro/tdl.html" target="_blank">TD-Gammon</a>.  Does backgammon have particular properties that make it easier for reinforcement learning and self-play to work?  Can these properties be exploited to design better general learning algorithms?  Follow-up experiments show domain stochasticity to have a strong impact on reinforcement learning with self-play.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/gamegraph/gamegraph.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/gamegraph/document.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="projects/gamegraph/proposal.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">proposal</span></a>
								</li>
								<li>
									<a href="https://github.com/rezama/gamegraph" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/active_learning/variance.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/active_learning/document.pdf">Optimizing Selection of Training Samples for Robotics Learning Problems</a><br>
							Preprint, 2011<br>
							<small class="authors">
								Reza Mahjourian, Peter Stone
							</small>
							<br>
							<small class="abs">
								Uses an ensemble of neural networks and selects samples by prioritizing data points where the networks in the ensemble disagree the most about predictions (most variance).
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/active_learning/document.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="https://github.com/rezama/active-learning" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">github</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<br><br><br><br>
				<!-- ================================================================== -->

				<h3>
					Theoretical Computer Science
				</h3>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/bcast/heuristic.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/bcast/bcast.pdf">An Approximation Algorithm for Conflict-Aware Broadcast Scheduling in Wireless
								Ad Hoc Networks</a><br>
							The ACM International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc), 2008<br>
							<small class="authors">
								Reza Mahjourian, Feng Chen, Ravi Itwari, My Thai, Hongqiang Zhai, Yuguang Fang
							</small>
							<br>
							<small class="abs">
								This paper introduces and proves correctness of a constant approximation algorithm for minimum-latency conflict-aware broadcast scheduling in wireless networks. A constant approximation algorithm is a polynomial-time solution to an NP-hard problem such that the solution is within a constant multiple of the optimal solution to the problem.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/bcast/bcast.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/bcast/bcast.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<br><br><br><br>
				<!-- ================================================================== -->

				<h3>
					Software Engineering
				</h3>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/xpage/data.png" class="thumb-paper" />
						</th>
						<th class="entry">

							<a href="projects/xpage/icsr.pdf">An Architectural Style for Data-Driven Systems</a><br>
							International Conference on Software Reuse (ICSR), 2008<br>
							<small class="authors">
								Reza Mahjourian
							</small>
							<br>
							<small class="abs">
								This paper describes the design of <a href="https://xpage.sourceforge.net/" target="_blank">XPage</a>, a light-weight web application framework, which is also published as open-source software in 2002, and deployed in six data management apps by the author.  It is designed specifically for data management applications and allows the developer to specify each application page at a very high level by specifying the data sources and attributes that it retrieves or modifies.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/xpage/xpage.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="https://xpage.sourceforge.net/" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-globe fa-fw"></i>
										<span class="network-name">website</span></a>
								</li>
								<li>
									<a href="projects/xpage/icsr.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
								<li>
									<a href="projects/xpage/extended.pdf" targwhich transparently translates high-level web page specs into server-side s     et="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">extended</span></a>
								</li>
								<li>
									<a href="https://sourceforge.net/projects/xpage/" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-github fa-fw"></i>
										<span class="network-name">sourceforge</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

				<table class="entry">
					<tr>
						<th>
							<img width="170" src="projects/neno/connector.png" class="thumb-paper" />
						</th>
						<th class="entry">
							<a href="projects/neno/iwicss07.pdf">Software Connector Classification and Selection for Data-Intensive Systems</a><br>
							International Workshop on Incorporating COTS Software into Software Systems, 2007<br>
							<small class="authors">
								Chris A. Mattmann, David Woollard, Nenad Medvidovic, Reza Mahjourian
							</small>
							<br>
							<small class="abs">
								This work explores the role of software connectors in systems specifically designed for distributing large volumes of data.
							</small>
							<br>
							<ul class="list-inline list-inline-x text-left">
								<li>
									<a href="projects/neno/cite.bib" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-text-o fa-fw"></i>
										<span class="network-name">bibtex</span></a>
								</li>
								<li>
									<a href="projects/neno/iwicss07.pdf" target="_blank" class="btn-x btn-default-x btn-lg-x"><i class="fa fa-file-pdf-o fa-fw"></i>
										<span class="network-name">pdf</span></a>
								</li>
							</ul>
						</th>
					</tr>
				</table>

				<!-- ================================================================== -->

			</div>
		</div>
	</section>

	<!-- jQuery -->
	<script src="js/jquery.js"></script>

	<!-- Bootstrap Core JavaScript -->
	<script src="js/bootstrap.min.js"></script>

	<!-- Plugin JavaScript -->
	<script src="js/jquery.easing.min.js"></script>

	<!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/
    <script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCRngKslUGJTlibkQ3FkfTxj3Xss1UlZDA&sensor=false"></script>-->

	<!-- Custom Theme JavaScript -->
	<script src="js/grayscale.js"></script>


</body>

</html>
